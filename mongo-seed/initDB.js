use csynapse

// "Maxnet":{"description":"Maxnet", "name":"Maxnet","type":"supervised","paramInfo":[]},
// "Multilayer_Neural_Network_Linear":{"description":"multilayer neural network with linear loss", "name":"Multi-Layer Neural Network Linear","type":"supervised","paramInfo":[]},
// init algos collection
db.algorithms.insert({"_id":"algorithms","svm":{'tooltip':'Support Vector Machine (or SVM) is a supervised learning model that analyzes data used for classification and regression analysis. The algorithm trains on data to form two distinct groups of data points. New data will be classified based on which group the data point falls nearest to. <a href=""https://en.wikipedia.org/wiki/Support_vector_machine"">More info.</a>',"description":"Support Vector Machines", "name":"Support Vector Machine", "type":"supervised",
"paramInfo":[{"name":"C","default":1,"type":"float","greater":"0","lessOrEqual":"none", "description":"Penalty parameter C of the error term."}]},
"knearest":{'tooltip':'Data input is classified by the majority vote of its neighbors, the neighbors being training data. New data is classified by the properties of the neighbors it was placed by. <a href=""https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm"">More info.</a>',
"description":"Knearest Neighbors", "name":"K-nearest Neighbors", "type":"supervised",
"paramInfo":[{"name":"n_neighbors","default":5,"type":"int","greater":"0","lessOrEqual":"none", "description":"Number of neighbors to use by default for k_neighbors queries."},
{"name":"metric","default":'euclidean',"type":"set","values":["euclidean","manhattan","chebyshev","minkowski","wminkowski","seuclidean","mahalanobis"],"description":"The metric to use when calculating distance between instances in a feature array."}]},
"guassNB":{'tooltip':'A supervised learning algorithm based on the application of Bayes\' theorem, with the naive assumption of independence between every pair of features. The likellihood of the features is assumed to be Gaussian. <a href=""https://en.wikipedia.org/wiki/Naive_Bayes_classifier#Gaussian_naive_Bayes"">More info.</a>',"description":"Gaussian Naive Bayes", "name":"Gaussian Naive Bayes", "type":"supervised",
"paramInfo":[]},
"sgd":{'tooltip':'Similar to SVM, Stochastic Gradient Descent is a classification algorithm that iterates over the data repeatedly to find maximums and minimums. <a href=""https://en.wikipedia.org/wiki/Stochastic_gradient_descent"">More info.</a>',"description":"Stochastic Gradient Descent", "name":"Stochastic Gradient Descent","type":"supervised",
"paramInfo":[{"name":"alpha","default":0.0001,"type":"float","greater":"0","lessOrEqual":"none", "description":"Constant that multiplies the regularization term. Used to compute the learning rate."}]},
"adaBoost":{'tooltip':'A meta-classifier that first applies a classifier to the dataset. Additional copies of the classifier are added to the same dataset, but the weights of incorrectly classified instaces are adjusted so that subsequent classifiers focus on more difficult cases. <a href=""https://en.wikipedia.org/wiki/Adaboost"">More info.</a>',"description":"Adaboost: Decision Trees", "name":"Adabost", "type":"supervised",
"paramInfo":[{"name":"learning_rate","default":1,"type":"float","greater":"0","lessOrEqual":"none", "description":"Learning rate shrinks the contribution of each classifier by learning_rate. There is a trade-off between learning_rate and n_estimators."},
{"name":"n_estimators","default":50,"type":"int","greater":"0","lessOrEqual":"none", "description":"The maximum number of estimators at which boosting is terminated. In case of perfect fit, the learning procedure is stopped early."}]},
"randomForest":{'tooltip':'Random Forest is an algorithm that classifies through the creation of many decision trees during training. If used for classification, the algorithm outputs the mode of classes of the individual trees. For regression testing the algorithm outputs the mean prediction of the individual trees. <a href=""https://en.wikipedia.org/wiki/Random_forest"">More info.</a>',"description":"Random Forest", "name":"Random Forest","type":"supervised",
"paramInfo":[{"name":"criterion","default":"gini","type":"set","values":["gini","entropy"],"description":"The function to measure the quality of a split."},
{"name":"n_estimators","default":10,"type":"int","greater":"0","lessOrEqual":"none", "description":"The number of trees in the forest."}]},
"perceptron":{'tooltip':'A classification algorithm that makes predictions based on a linear predictor function combining a set of weights with the feature vector. <a href=""https://en.wikipedia.org/wiki/Perceptron"">More info.</a>',"description":"Perceptron", "name":"Perceptron","type":"supervised",
"paramInfo":[{"name":"penalty","default":"None","type":"set","values":["None","l2","l1","elasticnet"],"description":"The penalty (aka regularization term) to be used."},
{"name":"alpha","default":0.0001,"type":"float","greater":"0","lessOrEqual":"none", "description":"Constant that multiplies the regularization term if regularization is used."}]},
"nearestCentroid":{'tooltip':'Nearest Centroid is a classification algorithm that makes classifications by pairing the mean of the features of the input data to one that most fits the the training set. <a href=""https://en.wikipedia.org/wiki/Nearest_centroid_classifier"">More info.</a>',"description":"Nearest Centroid", "name":"Nearest Centroid","type":"supervised",
"paramInfo":[{"name":"metric","default":'euclidean',"type":"set","values":["euclidean","manhattan","chebyshev","minkowski","wminkowski","seuclidean","mahalanobis"],"description":"The metric to use when calculating distance between instances in a feature array."}]},
"passiveAggressive":{'tooltip':'A family of classification algorithm similar to Perceptron\'s. When predictions are made incorrectly, the model is adjusted to account for this incorrect classification. Contrary to the Perceptron, this family of algorithms do not require a learning rate. <a href=""http://jmlr.csail.mit.edu/papers/volume7/crammer06a/crammer06a.pdf"">More info.</a>',"description":"passiveAggressive", "name":"Passive Aggressive","type":"supervised",
"paramInfo":[{"name":"C","default":1,"type":"float","greater":"0","lessOrEqual":"none", "description":"Maximum step size (regularization)."}]},
"decisionTree":{'tooltip':'The Decision Tree algorithm is a supervised learning method used for classification and regression. The goal is to create a model that predicts the value of a target variable by learning decision rules inferred from the features of the dataset. <a href=""https://en.wikipedia.org/wiki/Decision_tree_learning"">More info.</a>',"description":"Decision Tree", "name":"Decision Tree","type":"supervised",
"paramInfo":[{"name":"criterion","default":"gini","type":"set","values":["gini","entropy"],"description":"The function to measure the quality of a split."}]},
"Adaline":{'tooltip':'The Adeline Neural Network is similar to the Multi-Layer Perceptron, save for a few differences. The Adaline Neural Network only has two layers.  The algorithm also uses weights, bias, and summations. During training these weights are adjusted according to the summation. <a href=""https://en.wikipedia.org/wiki/ADALINE"">More info.</a>',"description":"adaline neural network", "name": "Adaline Neural Net","type":"supervised","paramInfo":[]},
"Hebbian":{'tooltip':'A feed forward neural network. When two neurons fire at the same time, the weight between the nuerons increases. When the neurons fire at different times then the weight between them decreases. <a href=""https://en.wikipedia.org/wiki/Hebbian_theory"">More info.</a>',"description":"hebbian neural network", "name": "Hebbian Neural Net","type":"supervised","paramInfo":[]},
"Multilayer_Neural_Network_Gaussian":{'tooltip':'A multi-layer neural network that makes use of the Gaussian function loss. <a href=""https://en.wikipedia.org/wiki/Types_of_artificial_neural_networks"">More info.</a>',"description":"multilayer  neural network using gaussian loss", "name":"Multi-Layer Neural Net Guassian Loss","type":"supervised","paramInfo":[]},
"Multilayer_Neural_Network_sin":{'tooltip':'A multi-layer neural network that makes use of the sin function loss. <a href=""https://en.wikipedia.org/wiki/Types_of_artificial_neural_networks"">More info.</a>',"description":"multilayer neural network with sin loss", "name":"Multi-Layer Neural Network Sin","type":"supervised","paramInfo":[]},
"Multi_Neural_Network_Step":{'tooltip':'A multi-layer neural network that makes use of the step function loss. <a href=""https://en.wikipedia.org/wiki/Types_of_artificial_neural_networks"">More info.</a>',"description":"multilayer neural network with step loss", "name":"Multi-Layer Neural Network Step","type":"supervised","paramInfo":[]},
"layer":{'tooltip':'A neural network where inputs travel through neuron layers using weights to reach and output layer.',"description":"layered neural network", "name":"layered neural net","type":"supervised","paramInfo":[]},
"rbf_neural_network":{'tooltip':'A multi-layer neural network that makes use of the Radial Basis function. the predicted target value of a classification is likely to have similar values of the predictor variables. <a href=""https://en.wikipedia.org/wiki/Types_of_artificial_neural_networks"">More info.</a>',"description":"Radial Basis Neural Network", "name":"Radial Basis Neural Network","type":"supervised","paramInfo":[]},
"Tan":{'tooltip':'A multi-layer neural network that makes use of the tangent function loss. <a href=""https://en.wikipedia.org/wiki/Types_of_artificial_neural_networks"">More info.</a>',"description":"multilayer network with tan loss", "name":"Multi-Layer Neural Network Tan","type":"supervised","paramInfo":[]}})

// "leastSquares":{"description":"Least Squares", "name":"Least Square Regression","type":"regression"},
// "ridge":{"description":"Ridge Regression", "name":"Ridge Regression","type":"regression"},
// "lasso":{"description":"Lasso", "name": "Lasso","type":"regression"},
// "elasticNet":{"description":"Elastic Net", "name": "Elastic Net","type":"regression"},
// "lars":{"description":"Lars", "name":"Lars","type":"regression"},
// "orthogonalMatchingPursuit":{"description":"Orthongonal Matching Pursuit", "name":"Orthongonal Matching Pursuit","type":"regression"},
// "bayesianRidge":{"description":"Bayesian Ridge Regression", "name":"Bayesian Ridge Regression","type":"regression"},
// "logisticRegression":{"description":"Logistic Regression", "name": "Logistic Regression","type":"regression"},